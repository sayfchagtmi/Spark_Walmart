{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Walmart_Stock') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Load the Walmart Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = spark.read.csv('data/walmart_stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|       _c0|               _c1|      _c2|      _c3|               _c4|     _c5|               _c6|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, we need to define the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock = spark\\\n",
    "        .read\\\n",
    "        .option('header', True)\\\n",
    "        .csv('data/walmart_stock.csv')\n",
    "stock.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-Create a new dataframe with a column called HV_Ratio that is the ratio of the High Price versus volume of stock traded for a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newStock = stock\\\n",
    "            .withColumn('HV_Ratio', col('High')/col('Volume'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|            HV_Ratio|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newStock.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock\\\n",
    "    .orderBy(col('High').desc())\\\n",
    "    .select('Date')\\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same with sql\n",
    "* Create sql table from stock named stockSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.createOrReplaceTempView(\"stockSQL\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2015-01-13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select Date from stockSQL\n",
    "where High = (select max(High) from stockSQL)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the work using sql and spark (both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-What is the mean of the Close column?\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Moyenne Close|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock\\\n",
    "    .agg(mean('Close').alias('Moyenne Close'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Moyenne Close|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select mean(Close) as `Moyenne Close` \n",
    "from stockSQL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-What is the max and min of the Volume column?\n",
    "* Saprk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Minimum  volume|Maximum  volume|\n",
      "+---------------+---------------+\n",
      "|       10010500|        9994400|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock\\\n",
    "    .agg(min('Volume').alias('Minimum  volume'),\\\n",
    "         max('Volume').alias('Maximum  volume'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Minimum  volume|Maximum  volume|\n",
      "+---------------+---------------+\n",
      "|       10010500|        9994400|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select  min(Volume) as `Minimum  volume`,\n",
    "        max(Volume) as `Maximum  volume`\n",
    "from stockSQL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9-How many days was the Close lower than 60 dollars?\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Nb Jours|\n",
      "+--------+\n",
      "|      81|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock\\\n",
    "    .filter(col('Close')<60)\\\n",
    "    .agg(count('*').alias('Nb Jours'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Nb Jours|\n",
      "+--------+\n",
      "|      81|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select  count(*) as `Nb Jours`\n",
    "from stockSQL\n",
    "where Close < 60\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-What percentage of the time was the High greater than 80 dollars ?(In other words, (Number of Days High>80)/(Total Days in the dataset))\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Percentage|\n",
      "+----------+\n",
      "|      8.43|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = stock\\\n",
    "    .filter(col('High')>80)\\\n",
    "    .agg(count('*'))\\\n",
    "    .collect()[0][0]\n",
    "stock\\\n",
    "    .agg(round(p/count('*')*100, 2)\\\n",
    "    .alias('Percentage')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Percentage|\n",
      "+----------+\n",
      "|      8.43|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select round(\n",
    "                (select count(*) from stockSQL\n",
    "                where High>80)\n",
    "                /\n",
    "                (count(*))\n",
    "                * 100\n",
    "                , 2\n",
    "            )\n",
    "as Percentage \n",
    "from stockSQL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-What is the max High per year?\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|  Maximum|\n",
      "+----+---------+\n",
      "|2012|77.599998|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2015|90.970001|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "byYear = stock\\\n",
    "            .withColumn('Year', substring('Date', 1, 4).cast('int'))\n",
    "byYear\\\n",
    "    .groupBy(col('Year'))\\\n",
    "    .agg(max('High').alias('Maximum'))\\\n",
    "    .orderBy('Year')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|  Maximum|\n",
      "+----+---------+\n",
      "|2012|77.599998|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2015|90.970001|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select  substring(Date, 1, 4) as `Year`, max(High) as `Maximum`\n",
    "from stockSQL\n",
    "group by Year \n",
    "order by Year\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12-What is the average Close for each Calendar Month? In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|Month|Average|\n",
      "+-----+-------+\n",
      "|    1|  71.45|\n",
      "|    2|  71.31|\n",
      "|    3|  71.78|\n",
      "|    4|  72.97|\n",
      "|    5|  72.31|\n",
      "|    6|   72.5|\n",
      "|    7|  74.44|\n",
      "|    8|  73.03|\n",
      "|    9|  72.18|\n",
      "|   10|  71.58|\n",
      "|   11|  72.11|\n",
      "|   12|  72.85|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "byMonth = stock\\\n",
    "            .withColumn('Month', substring('Date', 6, 2).cast('int'))\n",
    "byMonth\\\n",
    "    .groupBy(col('Month'))\\\n",
    "    .agg(round(mean('Close'), 2).alias('Average'))\\\n",
    "    .orderBy('Month')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|Month|Average|\n",
      "+-----+-------+\n",
      "|   01|  71.45|\n",
      "|   02|  71.31|\n",
      "|   03|  71.78|\n",
      "|   04|  72.97|\n",
      "|   05|  72.31|\n",
      "|   06|   72.5|\n",
      "|   07|  74.44|\n",
      "|   08|  73.03|\n",
      "|   09|  72.18|\n",
      "|   10|  71.58|\n",
      "|   11|  72.11|\n",
      "|   12|  72.85|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select  substring(Date, 6, 2) as `Month`, \n",
    "        round(mean(Close), 2) as `Average`\n",
    "from stockSQL\n",
    "group by Month \n",
    "order by Month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark but transforming months numbers to names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|Month|Average|\n",
      "+-----+-------+\n",
      "|  Oct|  71.58|\n",
      "|  Sep|  72.18|\n",
      "|  Dec|  72.85|\n",
      "|  Aug|  73.03|\n",
      "|  May|  72.31|\n",
      "|  Jun|   72.5|\n",
      "|  Feb|  71.31|\n",
      "|  Nov|  72.11|\n",
      "|  Mar|  71.78|\n",
      "|  Jan|  71.45|\n",
      "|  Apr|  72.97|\n",
      "|  Jul|  74.44|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Additional needed packages \n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "byMonth = stock\\\n",
    "            .withColumn('Month', substring('Date', 6, 2).cast('int'))\n",
    "\n",
    "month_lst = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: month_lst[int(x%12) - 1], StringType())\n",
    "byMonth = byMonth\\\n",
    "            .select(*[udf(column)\\\n",
    "            .alias('Month') if column == 'Month' else column for column in byMonth.columns])       \n",
    "\n",
    "byMonth\\\n",
    "    .groupBy(col('Month'))\\\n",
    "    .agg(round(mean('Close'), 2).alias('Average'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Don't forget to stop the session !</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
